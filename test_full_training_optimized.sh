#!/usr/bin/env bash

# Test full fine-tuning with disk space optimization
set -euo pipefail

echo "ğŸ§ª Testing Full Fine-tuning with Disk Space Optimization"
echo "========================================================="

# Activate conda environment
eval "$(conda shell.bash hook)"
conda activate llamafactory

# Check disk space
echo "ğŸ“Š Checking disk space..."
df -h .

# Clean up any previous test outputs
echo "ğŸ§¹ Cleaning up previous test outputs..."
rm -rf ./test_full_output_optimized
rm -rf ./test_c4_output
rm -rf ./test_bangla_small_output

echo "Running optimized full fine-tuning test..."
llamafactory-cli train examples/train_full/smollm2_bangla_pretrain_test.yaml \
    max_samples=20 \
    per_device_train_batch_size=1 \
    gradient_accumulation_steps=1 \
    num_train_epochs=1 \
    save_steps=20 \
    logging_steps=5 \
    output_dir=./test_full_output_optimized \
    save_only_model=true

if [ $? -eq 0 ]; then
    echo "âœ… Optimized full fine-tuning test PASSED!"
    echo ""
    echo "ğŸ‰ Full fine-tuning works perfectly!"
    echo ""
    echo "ğŸ“‹ Next steps:"
    echo "1. âœ… Full fine-tuning confirmed working"
    echo "2. âœ… LoRA training confirmed working" 
    echo "3. âœ… Small dataset training confirmed working"
    echo "4. ğŸ”§ Need to optimize disk space for larger datasets"
    echo ""
    echo "ğŸ’¡ Recommendations:"
    echo "- Use 'save_only_model=true' to save only model weights"
    echo "- Increase 'save_steps' to save checkpoints less frequently"
    echo "- Consider using 'save_total_limit' to limit number of checkpoints"
    echo "- Monitor disk space during training"
else
    echo "âŒ Optimized full fine-tuning test FAILED!"
    exit 1
fi
